{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anoushkabhat13/BCI_Project_Drowsiness_Detection/blob/main/BCI_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJVNfJzOHTKi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import mne\n",
        "from mne.datasets.sleep_physionet.age import fetch_data\n",
        "from mne.time_frequency import psd_welch\n",
        "from mne.io import Raw\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import scipy.signal as sig        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eeg = mne.datasets.sleep_physionet.age.fetch_data(list(range(0, 83)), recording=(1, 2), path=None, force_update=False, base_url='https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette/', on_missing='ignore',verbose=None)\n",
        "\n",
        "# get annotations and raw eeg files for data\n",
        "edf_array = []\n",
        "annotation_array = []\n",
        "\n",
        "for file in eeg:\n",
        "    # index 0 is the recording, index 1 is the annotations\n",
        "    #Fpz-Cz pre-frontal midline - central midline\n",
        "    # muse uses frontal lobe FP and temporal lobe \n",
        "    edf = mne.io.read_raw_edf(file[0], misc=[\"EEG Fpz-Cz\"])\n",
        "    annotation = mne.read_annotations(file[1])\n",
        "    edf.set_annotations(annotation, emit_warning=False)\n",
        "    # get the EEG signals\n",
        "    edf_array.append(edf)\n",
        "    annotation_array.append(annotation)"
      ],
      "metadata": {
        "id": "4KTLZNMv6YAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gets data in array format (similar to csv file) and runs preprocessing\n",
        "raw_data = []\n",
        "for i in range(0, len(edf_array)):\n",
        "    print(i)\n",
        "    data = edf_array[i].get_data()[0]\n",
        "    raw_data.append(preprocessing.scale(data))\n",
        "print(raw_data)"
      ],
      "metadata": {
        "id": "qMcleL9M62i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataframe - 2 columns - first colume dataframe with all eeg data, second one time points of events, third one annotations\n",
        "sleep_stages = {'Sleep stage W': 0,'Sleep stage 1': 1}\n",
        "all_data = pd.DataFrame()\n",
        "full_df = list()\n",
        "\n",
        "for i in range(len(raw_data)):\n",
        "    events, event_ids = mne.events_from_annotations(edf_array[i], event_id=sleep_stages)\n",
        "    epochs = mne.Epochs(edf_array[i], events, event_ids, 0, 10, baseline=None, preload=True)\n",
        "    dataf = epochs.to_data_frame()\n",
        "    dataf.sort_index(inplace=True)\n",
        "    dataf = dataf.drop(columns=['EEG Pz-Oz', 'EOG horizontal', 'Resp oro-nasal',\n",
        "       'EMG submental', 'Temp rectal', 'Event marker'])\n",
        "    dataf['condition'].replace({\"Sleep stage W\": \"0\", \"Sleep stage 1\": \"1\"}, inplace=True)\n",
        "    subject = [i]*dataf.shape[0]\n",
        "    dataf['subject'] = subject\n",
        "    full_df.append(dataf)\n",
        "    \n",
        "    all_data = pd.concat([all_data, dataf], axis=0)"
      ],
      "metadata": {
        "id": "6qQzIcNo6pHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = all_data.groupby(\"subject\")\n",
        "sig_0 = [[]]\n",
        "sig_1 = [[]]\n",
        "\n",
        "for i in range(len(grouped)):\n",
        "    # group by subject\n",
        "    subject_grouped = grouped.get_group(i)\n",
        "    \n",
        "    # group subject data by condition\n",
        "    sub_grouped = subject_grouped.groupby(\"condition\")\n",
        "    subgrp_0 = sub_grouped.get_group(\"0\")\n",
        "    subgrp_1 = sub_grouped.get_group(\"1\")\n",
        "    \n",
        "    sig_0.append(subgrp_0[\"EEG Fpz-Cz\"].values[:10])\n",
        "    sig_1.append(subgrp_1[\"EEG Fpz-Cz\"].values[:10])\n",
        "\n",
        "sig_0.pop(0)\n",
        "sig_1.pop(0)\n",
        "\n",
        "total_array = sig_0 + sig_1\n",
        "events_array = ([0]*len(grouped)) + ([1]*len(grouped))\n",
        "new_data = {\"EEG Signal\": total_array, \"Events\": events_array}\n",
        "new_dataframe = pd.DataFrame(new_data)\n",
        "new_dataframe\n",
        "\n",
        "X = new_dataframe[\"EEG Signal\"]\n",
        "y = new_dataframe[\"Events\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)"
      ],
      "metadata": {
        "id": "cuvSyLF07K7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc78krbBqun0"
      },
      "outputs": [],
      "source": [
        "a = new_dataframe.iloc[:,0] # the eeg signals\n",
        "#print(new_dataframe.iloc[:, 1])\n",
        "b = new_dataframe.iloc[:, 1]\n",
        "A_train, A_test, b_train, b_test = train_test_split(a, b, test_size =0.3, random_state=42)\n",
        "\n",
        "# make the data used in the model into 3d array\n",
        "xarr = np.array(total_array)\n",
        "xarr.shape #200 samples 50 epochs per sample 1 channel\n",
        "newx = xarr[:, :, np.newaxis] # make to 3d\n",
        "\n",
        "#feature selection\n",
        "import scipy.stats\n",
        "import entropy as ent\n",
        "import tsfel\n",
        "def mean(x):\n",
        "    return np.mean(x, axis=None)\n",
        "def abs_diff_signal(x):\n",
        "    return np.sum(np.abs(np.diff(x, axis=-1)), axis=None)\n",
        "def std(x):\n",
        "    return np.std(x, axis=None)\n",
        "def rms(x):\n",
        "    return np.sqrt(np.mean(x**2, axis=None))\n",
        "def ptp(x):\n",
        "    return np.ptp(x, axis=None)\n",
        "def ent(x):\n",
        "    pd_series = pd.Series(x)\n",
        "    p_data = pd_series.value_counts()\n",
        "    return scipy.stats.entropy(p_data)\n",
        "def psd(x):\n",
        "    freqs, psd = signal.welch(x, 100, nperseg = 400)\n",
        "    freq_res = freqs[1]-freqs[0]\n",
        "\n",
        "gmean = []\n",
        "gstd = []\n",
        "grms = []\n",
        "gent = []\n",
        "gptp = []\n",
        "for d in newx:\n",
        "    gmean.append(mean(d))\n",
        "    gstd.append(std(d))\n",
        "    grms.append(rms(d))\n",
        "    gent.append(ent(d.flatten()))\n",
        "    gptp.append(ptp(d))\n",
        "pwr = []\n",
        "dis = []\n",
        "for d in xarr:\n",
        "    pwr.append(tsfel.feature_extraction.features.max_power_spectrum(d, 100))\n",
        "    dis.append(tsfel.feature_extraction.features.pk_pk_distance(d))\n",
        "\n",
        "new_dataframe[\"mean\"] = gmean\n",
        "new_dataframe[\"std\"] = gstd\n",
        "new_dataframe[\"ptp\"] = gptp\n",
        "new_dataframe[\"rms\"] = grms\n",
        "new_dataframe[\"ent\"] = gent\n",
        "new_dataframe[\"pwr\"] = pwr\n",
        "new_dataframe[\"dis\"] =  dis\n",
        "print(new_dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, n_jobs=1)\n",
        "\n",
        "#X = new_dataframe[[\"mean\", \"std\", \"dis\", \"rms\", \"ent\", \"ptp\", \"pwr\"]]\n",
        "\n",
        "# Build step forward feature selection\n",
        "sfs1 = SFS(clf,\n",
        "           k_features=4,\n",
        "           forward=True,\n",
        "           floating=False,\n",
        "           verbose=2,\n",
        "           scoring='accuracy',\n",
        "           cv=2)\n",
        "\n",
        "# Perform SFFS\n",
        "sfs1 = sfs1.fit(X_train, y_train)\n",
        "print(sfs1.k_feature_idx_)"
      ],
      "metadata": {
        "id": "Pfuzwc_MkJKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U06gJ6XG-Qj"
      },
      "outputs": [],
      "source": [
        "def check_column_score(cols):\n",
        "    '''\n",
        "    Trains and evaluates the model via cross-validation on the columns\n",
        "    of the dataset with select indices.\n",
        "    '''\n",
        "    \n",
        "    print(\"training with columns \" + str(cols))\n",
        "\n",
        "    SVM = svm.SVC()\n",
        "    return cross_val_score(SVM, X_train[cols], y_train, cv = 5).mean() \n",
        "    # average CV score for each subset of the predictor data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_confusion_matrix(score):\n",
        "    plt.figure(figsize=(9,9))\n",
        "    sns.heatmap(c, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
        "    plt.title(all_sample_title, size = 15)"
      ],
      "metadata": {
        "id": "aPh7dpXgkW7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(200)\n",
        "powers = 20\n",
        "gamma_pool = [10**k for k in range(-powers, powers+1)]\n",
        "# Initialize the score of the highest-performing model to negative infinity\n",
        "best_score = -np.inf\n",
        "best_g = -np.inf\n",
        "# Loop through each potential value for gamma, create an SVM model with that\n",
        "# gamma value, cross-validate that model on the training set, and save the\n",
        "# best score among all of these models\n",
        "#X = new_dataframe.loc[:, [\"mean\", \"std\", \"rms\"]].values\n",
        "X = new_dataframe.loc[:, [\"mean\", \"std\", \"rms\", \"dis\"]].values\n",
        "y = new_dataframe[\"Events\"].to_list()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
        "\n",
        "\n",
        "for g in gamma_pool:\n",
        "    SVM = svm.SVC(gamma = g)\n",
        "    score = cross_val_score(SVM, X_train, y_train, cv = 5).mean()\n",
        "    \n",
        "    # If the average CV score for an SVM model with the current gamma is\n",
        "    # greater than the best average CV score so far, make it the\n",
        "    # new best average CV score and store its gamma value\n",
        "    if score > best_score:\n",
        "        # Perform update\n",
        "        best_score = score\n",
        "        best_g = g\n",
        "\n",
        "# SEE OPTIMAL GAMMA VALUE AND BEST CV SCORE\n",
        "print(best_g, best_score)\n",
        "\n",
        "# Use best_g to evaluate our model on the test set\n",
        "SVM = svm.SVC(gamma = best_g)\n",
        "# Train model on training set\n",
        "SVM.fit(X_train, y_train)\n",
        "# Score model on test set to see how it performs on unseen data\n",
        "SVM.score(X_test, y_test)\n",
        "\n",
        "# Save predictions our model makes about sleep stage in the test set\n",
        "y_test_pred = SVM.predict(X_test)\n",
        "\n",
        "# Import the confusion_matrix() function from the metrics package in sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_test_pred))\n",
        "score = metrics.accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# Evaluate a confusion matrix on the test set\n",
        "c = confusion_matrix(y_test, y_test_pred)\n",
        "c\n",
        "\n",
        "visualize_confusion_matrix(score)\n"
      ],
      "metadata": {
        "id": "vlzKH41qD8mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "np.random.seed(200)\n",
        "X = new_dataframe.loc[:, [\"mean\", \"std\", \"rms\", \"dis\"]].values\n",
        "y = new_dataframe[\"Events\"].to_list()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
        "\n",
        "N = 30\n",
        "\n",
        "scores = np.zeros(N)\n",
        "best_score = -np.inf\n",
        "\n",
        "for d in range(1, N+1):\n",
        "    RF3 = RandomForestClassifier(n_estimators=d)\n",
        "    scores[d-1] = cross_val_score(RF3, X_train, y_train, cv=5).mean()\n",
        "    if scores[d-1]>best_score:\n",
        "        best_score=scores[d-1]\n",
        "        best_N = d\n",
        "print(best_N, best_score)\n",
        "\n",
        "clf=RandomForestClassifier(n_estimators=best_N)\n",
        "\n",
        "\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "y_pred=clf.predict(X_test)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "HKtoo1E2kTp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CkqHQsDfzaGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combos = [[\"mean\"], [\"mean\", \"std\"], [\"mean\", \"std\", \"rms\"], [\"mean\", \"std\", \"dis\", \"ptp\", \"rms\"],\n",
        "          [\"mean\", \"std\", \"dis\", \"ptp\", \"rms\", \"ent\"], [\"mean\", \"std\", \"rms\", \"dis\"], [\"mean\", \"std\", \"rms\", \"pwr\"], [\"mean\", \"std\", \"rms\", \"dis\", \"ent\"]]\n",
        "X = new_dataframe[[\"EEG Signal\", \"mean\", \"std\", \"ptp\", \"rms\", \"ent\", \"dis\", \"pwr\"]]\n",
        "y = new_dataframe[\"Events\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
        "# not all possible combos, but a good starting point\n",
        "# Use a function?\n",
        "\n",
        "# Check the model score when trained on each subset of the predictor data\n",
        "for combo in combos:\n",
        "    x = check_column_score(combo)\n",
        "    print(\"CV score is \" + str(np.round(x,3)))\n",
        "    \n",
        "#mean/std/rms or mean/std/rms/dis looking like the highest"
      ],
      "metadata": {
        "id": "PhPZlx9m6KaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for checking which model is best -- if you have better code that is welcome too\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression()))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))"
      ],
      "metadata": {
        "id": "MhLj_xcxLE2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "#X = new_dataframe.loc[:, [\"EEG Signal\", \"mean\", \"std\", \"rms\"]].values\n",
        "X = new_dataframe.loc[:, [\"mean\", \"std\", \"rms\"]].values\n",
        "y = new_dataframe[\"Events\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
        "\n",
        "\n",
        "for name, model in models:\n",
        "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
        "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparison between different MLAs')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ELHPMfcRLHlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten,Conv1D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import BatchNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "#X = new_dataframe.loc[:, [\"mean\", \"std\", \"rms\"]].values\n",
        "X = new_dataframe[\"EEG Signal\"].values\n",
        "y = new_dataframe[\"Events\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
        "\n",
        "mylist = []\n",
        "for val in X_train:\n",
        "    l1 = []\n",
        "    for v in val:\n",
        "        l1.append([v])\n",
        "    mylist.append(l1)\n",
        "print(len(mylist), len(mylist[0]), len(mylist[0][0]))\n",
        "\n",
        "X_train = np.asarray(np.array(mylist)).astype(np.float32)\n",
        "\n",
        "mylist2 = []\n",
        "for val in X_test:\n",
        "    l2 = []\n",
        "    for v in val:\n",
        "        l2.append([v])\n",
        "    mylist2.append(l2)\n",
        "\n",
        "X_test = np.asarray(np.array(mylist2)).astype(np.float32)\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(Conv1D(32, kernel_size=3,activation='relu',input_shape=(24,1),padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv1D(64, kernel_size=3,activation='relu',input_shape=(24,1),padding='same'))\n",
        "model.add(Conv1D(64, kernel_size=3,activation='relu',input_shape=(24,1),padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(128, kernel_size=3,activation='relu',input_shape=(24,1),padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Conv1D(128, kernel_size=3,activation='relu',input_shape=(24,1),padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#model.summary()\n",
        "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "classifier = model.fit(X_train, y_train, epochs=24, batch_size=214)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
        "\n"
      ],
      "metadata": {
        "id": "JRN9TP8LBL1n"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BCI Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}